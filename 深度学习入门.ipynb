{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731c408d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T14:25:41.520777Z",
     "start_time": "2021-11-21T14:25:04.178011Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbbeb375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T14:27:03.070173Z",
     "start_time": "2021-11-21T14:27:03.001217Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    第一步：选择模型\n",
    "'''\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44848ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   第二步：构建网络层\n",
    "'''\n",
    "model.add(Dense(500,input_shape=(784,))) # 输入层，28*28=784  \n",
    "model.add(Activation('tanh')) # 激活函数是tanh \n",
    "#model.add(activation('sigmoid')) # 激活函数是sigmoid\n",
    "\n",
    "model.add(Dropout(0.5)) # 采用50%的dropout\n",
    " \n",
    "model.add(Dense(500)) # 隐藏层节点500个  \n",
    "model.add(Activation('tanh'))  \n",
    "model.add(Dropout(0.5))\n",
    " \n",
    "model.add(Dense(10)) # 输出结果是10个类别，所以维度是10  \n",
    "model.add(Activation('softmax')) # 最后一层用softmax作为激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd86d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "第三步：编译\n",
    "1.定义损失函数：loss = ('cateqorical crossentropy') \n",
    "2.优化器：optimizer \n",
    "3.指标：metrics \n",
    "'''\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) # 优化函数，设定学习率（lr）等参数  \n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, class_mode='categorical') # 使用交叉熵作为loss函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d51562",
   "metadata": {},
   "source": [
    "我们在做梯度下降和深度学习时，我们并不是真的最小化总损失,我们会把训练数据随机分成几个mini-batch。 具体步骤：\n",
    "\n",
    "- 随机初始化神经网络的参数 (跟梯度下降一样)\n",
    "- 先随机选择第一个batch出来,对选择出来的batch里面total loss, 计算偏微分，根据{L}'L′去更新参数\n",
    "- 然后随机选择第二个batch ，对第二个选择出来的batch里面total loss 计算偏微分，根据{L}''L′′更新参数\n",
    "- 反复上述过程，直到把所有的batch都统统过一次，一个epoch才算结束。 注意：假设今天有100个batch的话，就把这个参数更新100次，把所有的batch都遍历过叫做一个epoch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "'''\n",
    "   第四步：训练\n",
    "   .fit的一些参数\n",
    "   batch_size：对总的样本数进行分组，每组包含的样本数量\n",
    "   epochs ：训练次数\n",
    "   shuffle：是否把数据随机打乱之后再进行训练\n",
    "   validation_split：拿出百分之多少用来做交叉验证\n",
    "   verbose：屏显模式 0：不输出  1：输出进度  2：输出每次的训练结果\n",
    "'''\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # 使用Keras自带的mnist工具读取数据（第一次需要联网）\n",
    "# 由于mist的输入数据维度是(num, 28, 28)，这里需要把后面的维度直接拼起来变成784维  \n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2]) \n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])  \n",
    "Y_train = (numpy.arange(10) == y_train[:, None]).astype(int) \n",
    "Y_test = (numpy.arange(10) == y_test[:, None]).astype(int)\n",
    " \n",
    "model.fit(X_train,Y_train,batch_size=200,epochs=50,shuffle=True,verbose=0,validation_split=0.3)\n",
    "model.evaluate(X_test, Y_test, batch_size=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d9dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    第五步：输出\n",
    "\n",
    "'''\n",
    "print(\"test set\")\n",
    "scores = model.evaluate(X_test,Y_test,batch_size=200,verbose=0)\n",
    "print(\"\")\n",
    "print(\"The test loss is %f\" % scores)\n",
    "result = model.predict(X_test,batch_size=200,verbose=0)\n",
    " \n",
    "result_max = numpy.argmax(result, axis = 1)\n",
    "test_max = numpy.argmax(Y_test, axis = 1)\n",
    " \n",
    "result_bool = numpy.equal(result_max, test_max)\n",
    "true_num = numpy.sum(result_bool)\n",
    "print(\"\")\n",
    "print(\"The accuracy of the model is %f\" % (true_num/len(result_bool)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
